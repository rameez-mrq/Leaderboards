# CS7IS3 Assignment 2 - Evaluation & Leaderboard Guide

This guide shows you how to run the evaluation workflow and automatically submit your MAP scores to the leaderboard.

## ğŸ“‹ Required Files

Before running the evaluation, ensure your repository has these files and directories. You can download them from the links below:

### Required Files (Root Directory):
- âœ… `pom.xml` - Maven project configuration
  - ğŸ“¥ [Download pom.xml](https://drive.google.com/file/d/1f3X7Q5PtiDjbemFU3Hv4heeMsrLEAZVC/view?usp=drive_link)
- âœ… `topics` - Search topics file
  - ğŸ“¥ [Download topics](https://drive.google.com/file/d/13FB15WEDbGO_cP1x9ule_pFZYqkbLYqp/view?usp=drive_link)
- âœ… `qrels.assignment2.part1` - Relevance judgments for evaluation
  - ğŸ“¥ [Download qrels.assignment2.part1](https://drive.google.com/file/d/1PzOTutqdPd4gwz37RyLjtSD64pN7mHDB/view?usp=drive_link)

### Required Files (Tools Directory):
- âœ… `tools/evaluate.py` - Python script for evaluating search results
  - ğŸ“¥ [Download evaluate.py](https://drive.google.com/file/d/1uCiJTSSP52dhOJsAnw9X2BTJEY6fOLZA/view?usp=drive_link)

### Required Files (GitHub Workflow):
- âœ… `.github/workflows/evaluation.yml` - Evaluation workflow file
  - ğŸ“¥ [Download evaluation.yml](https://drive.google.com/file/d/16-fZZqy-U6F8zzkJhuvafD6AJ6mhKaWZ/view?usp=drive_link)

### Required Directory:
- âœ… `Assignment Two/` - Dataset directory containing:
  - `fbis/` - FBIS documents
  - `fr94/` - Federal Register documents (subdirectories 01-12)
  - `ft/` - Financial Times documents (subdirectories ft911-ft944)
  - `latimes/` - LA Times documents
  - `dtds/` - DTD files for document parsing
    - `fbisdtd.dtd`
    - `fr94dtd`
    - `ftdtd`
    - `latimesdtd.dtd`
  - ğŸ“¥ [Download Assignment Two dataset](https://drive.google.com/file/d/17HXFUsKifeCoSobvfgQuNv6ctPz6K0z9/view?usp=sharing)

> ğŸ’¡ **Tip:** After downloading, extract the `Assignment Two` directory to your repository root. For other files, place them in the exact locations shown in the [File Structure Reference](#-file-structure-reference) section below.

### Your Java Source Code:
- âœ… `src/main/java/App.java`
- âœ… `src/main/java/Indexer.java`
- âœ… `src/main/java/Searcher.java`

### Required for leaderboard submission:
- GitHub Secrets: `LEADERBOARD_API_URL` and `LEADERBOARD_API_TOKEN`
- Optional: `TEAM_NAME` - Your team name (if not set, repository name will be used)
- Optional: `TEAM_MEMBERS` - Names of team members, comma-separated (e.g., "John Doe, Jane Smith")

## ğŸš€ Quick Start

### Step 1: Configure GitHub Secrets (One-time setup)

To submit scores to the leaderboard, add these secrets to your repository:

1. Go to your repository â†’ **Settings** â†’ **Secrets and variables** â†’ **Actions**
2. Click **"New repository secret"** and add:

   **Secret 1:**
   - **Name:** `LEADERBOARD_API_URL`
   - **Value:** `https://leaderboard.qrameez.in`
   
   **Secret 2:**
   - **Name:** `LEADERBOARD_API_TOKEN`
   - **Value:** (provided by your instructor)
   
   **Secret 3 (Optional):**
   - **Name:** `TEAM_NAME`
   - **Value:** Your team name (e.g., "Team Lucene", "Query Rangers")
   - If not set, your repository name will be used as the team name
   
   **Secret 4 (Optional):**
   - **Name:** `TEAM_MEMBERS`
   - **Value:** Names of team members, comma-separated (e.g., "John Doe, Jane Smith, Bob Johnson")
   - If not set, your GitHub username will be used

> âš ï¸ **Note:** Without `LEADERBOARD_API_URL` and `LEADERBOARD_API_TOKEN`, the evaluation will still run but won't submit to the leaderboard.

### Step 2: Run the Evaluation

The evaluation runs automatically when you **push code** to your repository. You can also trigger it manually:

1. Go to **Actions** tab
2. Select **"CS7IS3 Assignment 2 - Search Engine Evaluation"**
3. Click **"Run workflow"** â†’ **"Run workflow"**

### Step 3: Check Results

1. **View workflow output:**
   - Go to **Actions** tab â†’ Click the latest workflow run
   - Check the **"Submit metrics to leaderboard"** step (should show âœ…)
   - View detailed metrics in the workflow summary

2. **View leaderboard:**
   - Visit: `https://leaderboard.qrameez.in`
   - Find your team and members' names with MAP score

## ğŸ“Š What the Evaluation Does

The workflow automatically:

1. **Validates** your project structure (checks for required files)
2. **Builds** your project (`mvn clean package`)
3. **Indexes** documents from `Assignment Two/` directory
4. **Searches** all topics from the `topics` file
5. **Evaluates** results using `qrels.assignment2.part1`
6. **Submits** scores to leaderboard (if secrets are configured)

## ğŸ“ Understanding Your Scores

The evaluation calculates these metrics:

- **MAP (Mean Average Precision)** - Primary ranking metric (0.0 to 1.0, higher is better)
- **P@5** - Precision at 5 (fraction of top 5 results that are relevant)
- **P@20** - Precision at 20 (fraction of top 20 results that are relevant)
- **nDCG@20** - Normalized DCG at 20 (ranking quality considering position)

**The leaderboard ranks by MAP score.**

## ğŸ”„ Updating Your Score

Every time you:
- Push new code
- Create a pull request
- Manually trigger the workflow

Your leaderboard entry **automatically updates** with the latest scores. No extra steps needed!

## ğŸ› Troubleshooting

### Workflow fails at "Build and Test Search Engine"

**Check:**
- âœ… `pom.xml` exists in root directory
- âœ… Java source files are in `src/main/java/`
- âœ… `topics` file exists in root directory
- âœ… `Assignment Two/` directory exists with dataset files

**Fix:** Add missing files and push again.

### Workflow fails at "Evaluate Results"

**Check:**
- âœ… `tools/evaluate.py` exists in your repository
- âœ… `qrels.assignment2.part1` file exists in root directory
- âœ… `runs/student.run` file was generated (check previous step logs)

**Fix:** Ensure `tools/evaluate.py` is present and the search step completed successfully.

### "Submit metrics to leaderboard" step is skipped

**Problem:** Step shows as gray (skipped)

**Solution:**
- Go to **Settings** â†’ **Secrets and variables** â†’ **Actions**
- Verify both `LEADERBOARD_API_URL` and `LEADERBOARD_API_TOKEN` are set
- Ensure URL is exactly: `https://leaderboard.qrameez.in` (no trailing slash)

### Scores show 0.0 on leaderboard

**Possible causes:**
- Search engine didn't produce results
- `runs/student.run` file is empty
- Build or search step failed

**Solution:**
- Check workflow logs in the **"Build and Test Search Engine"** step
- Verify your search engine produces output
- Ensure evaluation completed successfully

### Scores not appearing on leaderboard

**Check:**
1. Did "Submit metrics to leaderboard" step succeed? (âœ… green checkmark)
2. Wait 10-30 seconds and refresh the leaderboard page
3. Verify secrets are correct

**If still not working:**
- Check workflow logs for error messages
- Verify API URL in secrets matches: `https://leaderboard.qrameez.in`

## ğŸ“š File Structure Reference

Your repository should look like this:

```
your-repo/
â”œâ”€â”€ pom.xml
â”œâ”€â”€ topics
â”œâ”€â”€ qrels.assignment2.part1
â”œâ”€â”€ src/
â”‚   â””â”€â”€ main/
â”‚       â””â”€â”€ java/
â”‚           â”œâ”€â”€ App.java
â”‚           â”œâ”€â”€ Indexer.java
â”‚           â””â”€â”€ Searcher.java
â”œâ”€â”€ Assignment Two/
â”‚   â”œâ”€â”€ fbis/
â”‚   â”œâ”€â”€ fr94/
â”‚   â”‚   â”œâ”€â”€ 01/
â”‚   â”‚   â”œâ”€â”€ 02/
â”‚   â”‚   â””â”€â”€ ... (subdirectories 03-12)
â”‚   â”œâ”€â”€ ft/
â”‚   â”‚   â”œâ”€â”€ ft911/
â”‚   â”‚   â”œâ”€â”€ ft921/
â”‚   â”‚   â””â”€â”€ ... (other ft subdirectories)
â”‚   â”œâ”€â”€ latimes/
â”‚   â””â”€â”€ dtds/
â”‚       â”œâ”€â”€ fbisdtd.dtd
â”‚       â”œâ”€â”€ fr94dtd
â”‚       â”œâ”€â”€ ftdtd
â”‚       â””â”€â”€ latimesdtd.dtd
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ evaluate.py
â””â”€â”€ .github/
    â””â”€â”€ workflows/
        â””â”€â”€ evaluation.yml
```

## ğŸ¯ Best Practices

1. **Test locally first:**
   ```bash
   mvn clean package
   ```
   Fix build errors before pushing.

2. **Check workflow logs:**
   - Always review the Actions tab after pushing
   - Look for warnings or errors in each step

3. **Keep secrets secure:**
   - Never commit secrets to your code
   - Never share your `LEADERBOARD_API_TOKEN`

4. **Push frequently:**
   - The leaderboard shows your latest successful evaluation
   - Push after each improvement to update your score

## ğŸ”— Quick Reference

- **Leaderboard:** `https://leaderboard.qrameez.in`
- **Workflow:** `.github/workflows/evaluation.yml`
- **Secrets:** Settings â†’ Secrets and variables â†’ Actions
- **Actions Tab:** `https://github.com/YOUR_USERNAME/YOUR_REPO/actions`

---

**Good luck with your search engine implementation! ğŸš€**
